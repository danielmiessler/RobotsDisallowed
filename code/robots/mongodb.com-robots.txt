# See http://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file
#
# To ban all spiders from the entire site uncomment the next two lines:
# User-agent: *
# Disallow: /

User-agent: *
Crawl-delay: 10
# Directories
Disallow: /misc/
Disallow: /profiles/

# Files
Disallow: /admin/*
Disallow: /data/*
Disallow: /comment/reply/
Disallow: /filter/tips/
Disallow: /node/add/
Disallow: /search/
Disallow: /user/register/
Disallow: /user/password/
Disallow: /user/login/
Disallow: /user/logout/
Disallow: lp/p/*
Disallow: lp/s/*
Disallow: lp/d/*
Disallow: thank-you/*
# No access for quicktabs in the URL
Disallow: /*?quicktabs_*
Disallow: /*&quicktabs_*
Disallow: /*?qt-view_*
Disallow: /*&qt-view_*
# No crawling field collections or taxonomy
Disallow: /field-collection/*
Disallow: /tags/*
Disallow: /taxonomy/term/*
# Paths (no clean URLs)
Disallow: /?q=admin/
Disallow: /?q=comment/reply/
Disallow: /?q=filter/tips/
Disallow: /?q=node/add/
Disallow: /?q=search/
Disallow: /?q=user/password/
Disallow: /?q=user/register/
Disallow: /?q=user/login/
Disallow: /?q=user/logout/
# Specific pages
Disallow: /commercialsupport/
Disallow: /partners/referral-federal
Disallow: /legal/
Disallow: /customer-evaluation-downloads-development-versions